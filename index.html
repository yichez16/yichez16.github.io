<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta name=viewport content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    .new {
    background-color : #cc0000; color:white; border-radius : 5px; padding :1px; font-size : 14px; margin : 0 5px;}








    </style>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>Fabien Baradel</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-86789415-1', 'auto');
  ga('send', 'pageview');









    </script>
</head>
<body>
<table width="840" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle">
                        <p align="center">
                            <name>Yicheg Zhang</name>
                        </p>

                        <!-- ######## ABSTRACT #########-->


                        <p>
                            I am a Ph.D student in the <a href="https://aicps.eng.uci.edu/">Embedded & Cyber-Physical Systems Lab</a> and <a href="https://faculty.sites.uci.edu/zhouli/">DSP lab</a>,
                            at <a href="https://en.wikipedia.org/wiki/University_of_California,_Irvine">UC Irvine</a>, where I study Cyber-Physical System Security and machine learning security. 
                            I am co-advised by <a href="https://engineering.uci.edu/users/mohammad-al-faruque"></a> and <a href="https://faculty.sites.uci.edu/zhouli/">Zhou Li</a>.
                            I received B.S. in Electrical Engineering and Automation from <a href="https://en.wikipedia.org/wiki/Sichuan_University">Sichuan University</a>.

                        <p>
                            <!-- I am a research scientist at <a href="https://europe.naverlabs.com/">Naver Labs Europe</a> working on computer vision and machine learning.
                            I did my PhD at <a href="https://www.insa-lyon.fr">INSA Lyon</a>
                            advised by <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            and <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>.
                            I have also spent time at Google, Simon Fraser University and University of Guelph during my
                            PhD journey.
                            I received my Engineer's degree (MSc) from <a href="http://www.ensai.fr">ENSAI</a>.
                            My research interests focus on video understanding and deep learning. -->

                            

                        <!-- </p> -->

                        <!--<p>I am a third year PhD Candidate at <a href="https://www.insa-lyon.fr">INSA Lyon</a> - <a-->
                        <!--href="https://liris.cnrs.fr/?set_language=fr">LIRIS</a>.-->
                        <!--<br>-->
                        <!--I am working on <strong>Machine Learning</strong> and <strong>Computer Vision</strong>.-->
                        <!--<br>-->
                        <!--My supervisors are <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a> and <a-->
                        <!--href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>.-->
                        <!--<br>-->
                        <!--My PhD focuses on video understanding.-->
                        <!--<br>-->
                        <!--I am a Student Researcher Intern at <a href="https://ai.google/research/teams/perception/">-->
                        <!--Google Research</a> <br> in the Perception team under supervision on-->
                        <!--<a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>.-->
                        <!--</p>-->

                        <!--<p align="center">-->

                        <!--PhD thesis title:-->
                        <!--<br>-->
                        <!--<strong>"Deep Learning for Human Understanding:-->
                        <!--<br> poses, gestures, activities"</strong>-->
                        <!--<br>funded by the ANR/NSREC DeepVision project.-->
                        <!--</p>-->
                        <!--<p>-->
                        <!--I received my Engineer's degree (MSc) from <a href="http://www.ensai.fr">ENSAI</a> with a-->
                        <!--major in Data Science.-->
                        <!--Previously I've been intern at <a href="http://www.europe.naverlabs.com/">Xerox Research-->
                        <!--Centre-->
                        <!--Europe</a>.-->

                        <!--</p>-->


                        <!-- ######## INFO  #########-->

                        <p align=center>
                            <a href="yichez16<at>uci<dot>edu">Email</a> &nbsp/&nbsp
                            <a href="./docs/resume/Yicheng_CV.pdf">CV</a> &nbsp/&nbsp
                            <!-- <a href="https://scholar.google.fr/citations?user=egECWaEAAAAJ&hl=en">Scholar</a> &nbsp/&nbsp -->
                            <!-- <a href="https://github.com/fabienbaradel">Github</a> &nbsp/&nbsp -->
                            <a href="https://www.linkedin.com/in/yichez16/"> LinkedIn </a> &nbsp/&nbsp
                            <!-- <a href="https://twitter.com/fabienbaradel">Twitter</a> -->
                        </p>
                    </td>
                    <td width="33%">
                        <img src="./images/yicheng.JPG">
                    </td>
                </tr>
            </table>

            <!-- ######## NEWS  #########-->

            <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--<tbody>-->
            <!--<tr>-->
            <!--<td width="100%" valign="middle">-->
            <!--<heading id="news">News</heading>-->
            <!--<ul>-->
            <!--<li>-->
            <!--December 2019: One paper accepted to ICLR'20 as a spotlight presentation!-->
            <!--</li>-->
            <!--<li>September 2019: New paper on arXiv! <a-->
            <!--href="https://arxiv.org/abs/1909.12000">"CoPhy: Counterfactual Learning of Physical-->
            <!--Dynamics"</a>-->
            <!--</li>-->
            <!--<li>June 2019: New paper on arXiv: <a-->
            <!--href="https://arxiv.org/abs/1906.05743">"Learning Video Representations using-->
            <!--Contrastive Bidirectional Transformer"</a>-->
            <!--</li>-->
            <!--<li>April 2019: I am starting an internship at Google in Grenoble under supervision of <a-->
            <!--href="https://thoth.inrialpes.fr/~schmid/">Dr. Cordelia Schmid</a>-->
            <!--</li>-->
            <!--<li>July 2018: One paper accepted to BMVC'18 and one paper accepted to ECCV'18 !-->
            <!--</li>-->
            <!--<li>June 2018: New paper on arXiv - <a-->
            <!--href="https://arxiv.org/abs/1806.06157">"Object level visual reasoning in-->
            <!--videos"</a>-->
            <!--</li>-->
            <!--<li>February 2018: One paper accepted to CVPR'18 !-->
            <!--</li>-->
            <!--<li>February 2018: I will spend 4 months at <a-->
            <!--href="http://vml.cs.sfu.ca">Vision & Media lab</a> in Simon Fraser University under-->
            <!--direction of <a-->
            <!--href="http://www.cs.sfu.ca/~mori/">Dr. Greg Mori</a>-->
            <!--</li>-->
            <!--&lt;!&ndash;<li>August 2017: Two papers accepted to ICCVW'17 (<a&ndash;&gt;-->
            <!--&lt;!&ndash;href="http://adas.cvc.uab.es/task-cv2017/">TASK-CV</a> - <a&ndash;&gt;-->
            <!--&lt;!&ndash;href="http://icvl.ee.ic.ac.uk/hands17/">HANDS</a>)&ndash;&gt;-->
            <!--&lt;!&ndash;</li>&ndash;&gt;-->
            <!--&lt;!&ndash;<li>Summer 2017: I spend the summer at University of Guelph under direction of <a&ndash;&gt;-->
            <!--&lt;!&ndash;href="http://www.uoguelph.ca/~gwtaylor/">Dr. Graham Taylor</a>&ndash;&gt;-->
            <!--&lt;!&ndash;</li>&ndash;&gt;-->
            <!--&lt;!&ndash;<li>June 2017: I am participating at the <a&ndash;&gt;-->
            <!--&lt;!&ndash;href="https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/">MILA&ndash;&gt;-->
            <!--&lt;!&ndash;Deep & Reinforcement&ndash;&gt;-->
            <!--&lt;!&ndash;Learning Summer School</a> in Montreal!&ndash;&gt;-->
            <!--&lt;!&ndash;</li>&ndash;&gt;-->
            <!--&lt;!&ndash;<li>March 2017: New arXiv paper! <a href="./pose_rgb_attention_human_action">&ndash;&gt;-->
            <!--&lt;!&ndash;"Pose-conditioned Spatio-Temporal Attention for Human Action&ndash;&gt;-->
            <!--&lt;!&ndash;Recognition" </a> </span>&ndash;&gt;-->
            <!--&lt;!&ndash;</li>&ndash;&gt;-->
            <!--&lt;!&ndash;<li>January 2017: Seminar on "Introduction to Deep Learning with Tensorflow" at ENSAI&ndash;&gt;-->
            <!--&lt;!&ndash;</li>&ndash;&gt;-->
            <!--</ul>-->
            <!--</td>-->
            <!--</tr>-->
            <!--</tbody>-->
            <!--</table>-->


            <!-- ######## RESEARCH INTERESTS  #########-->

            <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--<tr>-->
            <!--<td width="100%" valign="middle">-->
            <!--<heading>Research</heading>-->
            <!--<p>-->
            <!--My main researchs focus on video understanding (e.g. action recognition, human-object-->
            <!--interaction).-->
            <!--I am also interested into discovering causal concepts from video by leveraging the arrow of-->
            <!--time.-->
            <!--<br/>-->
            <!--Keywords: machine learning, deep learning, computer vision, video understanding.-->
            <!--</p>-->
            <!--</td>-->
            <!--</tr>-->
            <!--</table>-->


            <!-- ######## PAPERS #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <td>
                    <heading>Publications</heading>
                </td>

                <tbody>

                <tr>
                    <td width="25%"><img src="images/cf.png" alt="blind-date"
                                         width="200"
                                         height="105"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/abs/1909.12000">
                                <papertitle>CoPhy: Counterfactual Learning of Physical Dynamics
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="https://nneverova.github.io/">Natalia Neverova</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
                            <a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>ICLR</em>, 2020 &nbsp; <font color="red"><strong>(Spotlight
                            presentation)</strong></font>
                            <!--<em>International Conference on Learning Representations-->
                            <!--(<strong>ICLR</strong>)</em>, 2020 <strong>(spotlight)</strong>-->
                            <br>
                            <a href="https://arxiv.org/pdf/1909.12000.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1909.12000">arXiv</a>
                            /
                            <a href="https://github.com/fabienbaradel/cophy">Code-Dataset</a>
                            /
                            <a href="https://youtu.be/95nqaDV9cYM">Video</a>
                            /
                            <a href="./bib/BaradelCophy_ICLR_20.txt">bibtex</a>
                        </p>
                        <p> We introduce a new problem of counterfactual learning of object mechanics from visual input
                            and a benchmark called CoPhy.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/cbt.png" alt="blind-date"
                                         width="200"
                                         height="155"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/abs/1906.05743">
                                <papertitle>Learning Video Representations using Contrastive Bidirectional Transformer
                                </papertitle>
                            </a>
                            <br>
                            <a href="http://chensun.me/">Chen Sun</a>,
                            <strong>Fabien Baradel</strong>,
                            <a href="https://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>,
                            <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>
                            <br>
                            <em>arXiv preprint</em>, 2019
                            <!--<em>arXiv</em>, 2019-->
                            <br>
                            <a href="https://arxiv.org/pdf/1906.05743.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1906.05743">arXiv</a>
                            /
                            <a href="./bib/SunBaradelMurphySchmid_arxiv_19.txt">bibtex</a>
                        </p>
                        <p> Self-supervised video representation by leveraging ASR and long videos via noise contrastive
                            estimation.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/arxiv18_objectLevelVisualReasoning.png" alt="blind-date"
                                         width="200"
                                         height="155"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/abs/1806.06157">
                                <papertitle>Object Level Visual Reasoning in Videos
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="https://nneverova.github.io/">Natalia Neverova</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
                            <a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>
                            <br>
                            <em>ECCV</em>, 2018
                            <!--<em>The IEEE European Conference in Computer Vision-->
                            <!--(<strong>ECCV</strong>)</em>, 2018-->
                            <br>
                            <a href="./eccv18_object_level_visual_reasoning">Project page</a>
                            /
                            <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fabien_Baradel_Object_Level_Visual_ECCV_2018_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1806.06157">arXiv</a>
                            /
                            <a href="./">video</a>
                            /
                            <a href="./bib/BaradelNeverovaWolfMilleMori_objectLevelVisualReasoning_ECCV_18.txt">bibtex</a>
                            /
                            <a href="https://github.com/fabienbaradel/object_level_visual_reasoning">Code</a>
                            /
                            <a href="./masks_data">Complementary Mask Data</a>
                            /
                            <a href="./docs/poster/ECCV2018.pdf">Poster</a>
                        </p>
                        <p> A model capable of learning to reason about semantically meaningful spatio-temporal
                            interactions in videos.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/bmvc18_hands.png" alt="blind-date" width="200" height="100"></td>
                    <td width="75%" valign="top">
                        <p>
                            <a href="./">
                                <papertitle> Human Activity Recognition with Pose-driven Attention to RGB
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>BMVC</em>, 2018
                            <!--<em>The British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2018-->
                            <br>
                            <a href="./papers/BMVC_18.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelWolfMille_BMVC_18.txt">bibtex</a>
                            /
                            <a href="./docs/poster/BMVC2018.pdf">Poster</a>
                        </p>
                        <p> Human activity recogntion using skeleton data and RGB. We propose a network able to focus on
                            relevant parts of the RGB stream given deep features extracted from the pose stream.
                        </p>
                    </td>
                </tr>


                <tr>
                    <td width="25%"><img src="images/cvpr18_glimpseclouds.png" alt="blind-date" width="200"
                                         height="155"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf">
                                <papertitle>Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
                            <a href="http://www.uoguelph.ca/~gwtaylor/">Graham Taylor</a>
                            <br>
                            <em>CVPR</em>, 2018
                            <!--<em>The IEEE Conference on Computer Vision and Pattern Recognition-->
                            <!--(<strong>CVPR</strong>)</em>, 2018-->
                            <br>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1802.07898">arXiv</a>
                            /
                            <a href="./cvpr18_glimpseclouds">project page</a>
                            /
                            <a href="https://youtu.be/7yPDYYhaYI4">video</a>
                            /
                            <a href="./bib/BaradelWolfMilleTaylor_CVPR_GlimpseClouds_18.txt">bibtex</a>
                            /
                            <a href="https://www.rsipvision.com/CVPR2018-Tuesday/14/">CVPR Daily</a>
                            /
                            <a href="https://github.com/fabienbaradel/glimpse_clouds">Code</a>
                            /
                            <a href="./docs/poster/CVPR2018.pdf">Poster</a>
                        </p>
                        <p> We propose a new method for human action recognition relying on RGB data only.
                            A visual attention module is able to extract glimpses within each frame.
                            Resulting local descriptors are soft-assigned to distributed workers which are finally
                            classifying the video.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/iccv17_hands.png" alt="blind-date" width="200" height="190"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w11/Baradel_Human_Action_Recognition_ICCV_2017_paper.pdf">
                                <papertitle>Human Action Recognition: Pose-based Attention draws focus to Hands
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>ICCV, Workshop "Hands in Action"</em>, 2017
                            <!--<em>The IEEE International Conference on Computer Vision (<strong>ICCV</strong>), Workshop-->
                            <!--"Hands in Action"</em>, 2017-->
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w11/Baradel_Human_Action_Recognition_ICCV_2017_paper.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelWolfMille_ICCV_Hands_17.txt">bibtex</a>
                            /
                            <a href="./docs/poster/ICCVW2017.pdf">Poster</a>
                        </p>
                        <p>A new spatio-temporal attention based mechanism for human action recognition able to
                            automatically attend to most important human hands and detect the most discriminative
                            moments in an action.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/iccv17_taskCV.png" alt="blind-date" width="200" height="95"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w38/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.pdf">
                                <papertitle>Discrepancy-based networks for unsupervised domain adaptation: a comparative
                                    study
                                </papertitle>
                            </a>
                            <br>
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Gabriela-Csurka">Gabriela
                                Csurka</a>,
                            <strong>Fabien Baradel</strong>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Boris-Chidlovskii">Boris
                                Chidlovskii</a>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Stephane-Clinchant">Stephane
                                Clinchant</a>,
                            <br>
                            <em>ICCV, Workshop "Task-CV"</em>, 2017
                            <!--<em>The IEEE International Conference on Computer Vision (<strong>ICCV</strong>), Workshop-->
                            <!--"Task-CV"</em>, 2017-->
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w38/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.pdf">PDF</a>
                            /
                            <a href="./bib/CsurkaBaradelChidlovskiiClinchant_ICCV_TaskCV_17.txt">bibtex</a>
                        </p>
                        <p>We introduce a new dataset for Domain Adaptation and show a comparaison between shallow and
                            deep methods based on Maximum Mean Discrepancy.
                        </p>
                    </td>
                </tr>


                <tr>
                    <td width="25%"><img src="images/attention_ntu.gif" alt="blind-date" width="160" height="160"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/pdf/1703.10106.pdf">
                                <papertitle>Pose-conditioned Spatio-Temporal Attention for Human Action Recognition
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>arXiv preprint</em>, 2017
                            <br>
                            <a href="https://arxiv.org/abs/1703.10106">arXiv</a>
                            /
                            <a href="https://arxiv.org/pdf/1703.10106.pdf">PDF</a>
                            /
                            <a href="./pose_rgb_attention_human_action">project page</a>
                            /
                            <a href="https://youtu.be/kGLGgH_VAZw">video</a>
                            /
                            <a href="./bib/BaradelWolfMille_PoseAttention_arXiv_17.txt">bibtex</a>
                        </p>
                        <p>We introduce an attention-based mechanism around hands on RGB videos conditioned on features
                            extracted from human 3D
                            pose.
                        </p>
                    </td>
                </tr>

                </tbody>

                <td>
                    <heading>PhD Thesis</heading>
                </td>

                <tr>
                    <td width="25%"><img src="images/phd_thesis.png" alt="blind-date" width="200" height="120"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="./docs/phd/manuscript.pdf">
                                <papertitle>Structured Deep Learning for Video Analysis
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>
                            <br>
                            <em>Universit&eacute; de Lyon - INSA Lyon</em>, 2020
                            <br>
                            <a href="./docs/phd/manuscript.pdf">PDF</a>
                            /
                            <a href="https://youtu.be/wOmUrfa6XiE">video</a>
                            /
                            <a href="./docs/phd/presentation.pdf">slides-pdf</a>
                            /
                            <a href="./docs/phd/presentation.pptx">slides-pptx</a>
                            /
                            <a href="./bib/Baradel_phdthesis_2020.txt">bibtex</a>
                        </p>
                    </td>
                </tr>

                <td>
                    <heading>Patents</heading>
                </td>

                <tr>
                    <td width="25%"><img src="images/cmmd_da.png" alt="blind-date" width="200" height="120"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://patents.google.com/patent/US20180253627A1/en">
                                <papertitle>Conditional adaptation network for image classification
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Gabriela-Csurka">Gabriela
                                Csurka</a>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Boris-Chidlovskii">Boris
                                Chidlovskii</a>,
                            <br>
                            <em>US Patent App. 15/450,620 - Xerox Corp</em>, 2017
                            <br>
                            <a href="https://patentimages.storage.googleapis.com/0e/64/38/504605db411662/US20180253627A1.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelCsurkaChidlovskii_USPatent_17.txt">bibtex</a>
                        </p>
                        <p>We introduce a new method based on Conditional Maximum Mean Discrepancy for domain adaptation
                            on image classification.
                        </p>
                    </td>
                </tr>


                <tbody>
            </table>

            <!--Reviewing activity-->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Reviewer</heading>
                        ECCV 2020, CVPR 2019-2020, ICML 2019, NIPS 2018, IJCV, TNNLS
                    </td>
                </tr>
            </table>

            <!-- ######## TEACHING #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="100%" valign="middle">
                        <heading id="news">Talks</heading>
                        <ul>
                            <li>
                                <a href="./">Counterfactual learning for physical dynamics</a> - ICML 2020 workshop "Object-Oriented Perception, Reasoning, and Causality" - July 2020
                            </li>
                            <li>
                                <a href="https://drive.google.com/file/d/1z6srYQP7VBwN8gDo4RT-1mAPWG0QFxOq/view?usp=sharing">Visual
                                    Reasoning in Videos</a> - GDR ISIS-IA "Reasoning from Visual Signal" - October 2018
                            </li>
                            <li>
                                <a href="https://drive.google.com/file/d/15QRxTqNfPfWex5wyrkjI0CdrDLH1xtlN/view?usp=sharing">Pose
                                    and Attention for Action Recognition</a> - GDR ISIS "Pose and Gesture" - December
                                2017
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>


            <!-- ######## TEACHING #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <heading>Teaching</heading>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
                <tr>
                    <td width="25%"><img src="./images/teaching.png" alt="teaching" width="160" height="160"></td>
                    <td width="75%" valign="center">
                        <p>
                            <a href="./">
                                <papertitle>Machine Learning</papertitle>
                            </a>
                            <br>
                            <em>Science U - M2 Info - 28h30 (CM+TP) - 2018/2019</em>
                            <br>
                            <a href="https://github.com/fabienbaradel/intro_ml">Github repo</a>
                            / Slides:
                            <a href="./docs/teaching/intro_ml/cours_1.pdf">1</a>
                            -
                            <a href="./docs/teaching/intro_ml/cours_2.pdf">2</a>
                            / Exercises:
                            <a href="./docs/teaching/intro_ml/tp_1.pdf">1</a>
                            -
                            <a href="./docs/teaching/intro_ml/tp_2.pdf">2</a>
                        </p>
                        <p>
                            <a href="./">
                                <papertitle>Regression Modelling</papertitle>
                            </a>
                            <br>
                            <em>Univ Lyon 1 - M2 Data Science - 12h (TP) - Fall 2017</em>
                        </p>
                        <p>
                            <a href="./">
                                <papertitle>Probability & Statistics</papertitle>
                            </a>
                            <br>
                            <em>Univ Lyon 1 - L2 Info & Maths-Eco - 12h+8h (TP) - Fall 2017</em>
                        </p>
                        <p>
                            <a href="./">
                                <papertitle>Mathematics</papertitle>
                            </a>
                            <br>
                            <em>EPITA - 1st year - 24h (CM+TD) - September 2017</em>
                        </p>
                        <p>
                            <a href="./images/tensorflow_ensai_SID_13_01_17.pdf">
                                <papertitle>Introduction to Deep Learning with Tensorflow</papertitle>
                            </a>
                            <br>
                            <em>ENSAI - MSc Data Science - 6h - January 2017</em>
                            <br>
                            <a href="https://github.com/fabienbaradel/Tensorflow-tutorials/">Github repo</a>
                            /
                            <a href="./docs/tensorflow_ensai_SID_13_01_17.pdf">slides</a>
                        </p>

                    </td>
                </tr>

            </table>


            <!-- ######## THANKS #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <br>
                        <p align="right">
                            <font size="2">
                                <a href="https://jonbarron.info">Awesome webpage...</a>
                            </font>
                        </p>
                    </td>
                </tr>
            </table>
            <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));














































































            </script>
            <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}













































































            </script>
        </td>
    </tr>
</table>
</body>
</html>

