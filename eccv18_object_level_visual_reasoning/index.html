<html>
<head>
    <title>Object level Visual Reasoning in Videos - LIRIS - INSA Lyon - ECCV'18</title>
    <meta name="google-site-verification" content="NksNPfO4SApMtvU2rGHxr4DPan2Uy6Pz-rP9cA0k1mg"/>
    <script type="text/javascript" src="jquery.js"></script>
    <script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
    <style>
body
{
    font-family : Arial;
	background-color : #f2f2f2;
    font-size : 16px;
}
hr {
    display: block;
    height: 1px;
    border: 0;
    border-top: 1px solid #ccc;
    margin: 1em 0;
    padding: 0;
}
/* Everything but the jumbotron gets side spacing for mobile first views */
.header,
.row,
.footer {
  padding-left: 15px;
  padding-right: 15px;
}
.content
{
    width : 900px;
    padding : 25px 25px;
    margin : 5px auto;
    background-color : #fff;
    border-radius: 20px;
}
.content-title {
    width : 900px;
    background-color : inherit;
    margin-bottom : 0;
    padding-bottom : 0;
}

a, a:visited
{
    color : blue;
}

#authors
{
    text-align : center;
}

#conference
{
    text-align : center;
    font-style : italic;
}

#authors span
{
    margin : 0 10px;
    display : inline-block;
}

h1
{
    font-family : Arial;
    font-size : 45px;
}
h2 {
    font-family : Arial;
    font-size : 30px;
    padding : 0; margin : 10px;
}
h3 {
    font-family : Arial;
    font-size : 15px;
    padding : 0; margin : 10px;
}

p {
    line-height : 130%;
    margin : 10px;
}
li {
    margin : 10px 0;
}

.samples {
    float : left;
    width : 50%;
    text-align : center;
}
.cond {
    float : left;
    margin : 0 40px;
}
.cond-container {
    width : 700px;
    margin : 0 auto;
    text-align : center;
}














































    </style>
</head>

<body>

<div class="content">
    <!--<center>-->
    <!--<a href="https://eccv2018.org/"><img-->
    <!--style="box-shadow: 0px 0px 0px #888888"-->
    <!--src="./eccv2018_logo.png" width="500px"></a>-->
    <!--</center>-->
    <div class="content-title">
        <h1>Object level Visual Reasoning in Videos</h1>
        <br>

    </div>


    <p id="authors">
        <span><a href="https://fabienbaradel.github.io"><img id="image_fb" src="../images/bio-photo.jpg"
                                                             height="100"><br>Fabien Baradel</a><br>INSA Lyon</span>
        <span><a href="https://nneverova.github.io"><img id="image_nn" src="../images/nneverova.jpg"
                                                         height="100"><br>Natalia Neverova</a><br>Facebook Research</span>
        <span><a href="http://liris.cnrs.fr/christian.wolf/"><img id="image_cw" src="../images/cwolf.jpg"
                                                                  height="100"><br>Christian Wolf</a><br>INRIA - INSA Lyon</span>
        <span><a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/"><img id="image_jm"
                                                                                 src="../images/jmille.jpg"
                                                                                 height="100"><br>Julien Mille</a><br>INSA Centre Val de Loire</span>
        <span><a href="http://www.cs.sfu.ca/~mori/"><img id="image_gm" src="../images/gmori.jpg"
                                                         height="100"><br>Greg Mori</a><br>Simon Fraser University</span><br
            clear="both">
        <br>
        <a href="../papers/ECCV_18.pdf"><img style="box-shadow: 5px 5px 2px #888888; margin: 10px"
                                             src="./paper-screenshot.png" width="150px"></a>
        <br>
        <strong> ECCV 2018 </strong>
    </p>

    <br>


    <h2>Abstract</h2>

    <p style="text-align: justify;">
        Human activity recognition is typically addressed by detecting key concepts like global and local motion,
        features related to object classes present in the scene, as well as features related to the global context.
        The next open challenges in activity recognition require a level of understanding that pushes beyond this and
        call for models with capabilities for fine distinction and detailed comprehension of interactions between actors
        and objects in a scene.
        We propose a model capable of learning to reason about semantically meaningful spatio-temporal interactions in
        videos.
        The key to our approach is a choice of performing this reasoning at the object level through the integration of
        state of the art object detection networks.
        This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction
        relevant level. We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC
        Kitchens) and achieve state of the art results on all of them.
        Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and
        their interactions corresponding to different activity classes.
    </p>

    <br>

    <center>
        <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fabien_Baradel_Object_Level_Visual_ECCV_2018_paper.pdf">
            <strong>Camera Ready paper</strong>
        </a>
        |
        <a href="https://arxiv.org/abs/1806.06157">
            <strong>arXiv version</strong>
        </a>
        |
        <a href="../masks_data/">
            <strong>Complementary Mask Data</strong>
        </a>
        |
        <a href="../docs/poster/ECCV2018.pdf">
            <strong>Poster</strong>
        </a>
        <br>
        <a href="https://github.com/fabienbaradel/object_level_visual_reasoning"><img
                style="box-shadow: 0px 0px 0px #888888; margin: 10px"
                src="../images/github.png" width="70px"></a>
    </center>
    <br>

    <center>
        <img src="./teaser.png" width="780">
        <p><i><strong>What happened between these two frames?</strong></i>
            <br>
            Humans have this extraordinary ability of performing visual reasoning on very complicated tasks while it
            remains unattainable for contemporary computer vision algorithms (Below a carrot was chopped by the human).
        </p>
    </center>
</div>


<br>

<hr/>

<div class="content">
    <h2>Object Relation Network</h2>
    <br>
    <center>
        <img src="./orn.png" width="780">
    </center>
    <p style="text-align: justify;">We propose an <strong>Object Relation Network</strong> (ORN), a neural network
        module for reasoning between
        detected semantic object instances through space and time.
        The ORN has potential to address these issues and conduct relational reasoning over object interactions for the
        purpose of activity recognition.
        A set of object detection masks ranging over different object categories and temporal occurrences is input to
        the ORN.
        The ORN is able to infer pairwise relationships between objects detected at varying different moments in time.
        ORN selects objects from different frames which have a semantic definition (e.g. a carrot) and is able of long
        range reasoning.
    </p>
</div>

<br>
<hr/>

<!--<div class="content">-->
<!--<h2>Vizualization</h2>-->
<!--<br>-->
<!--<center>-->
<!--<img src="./failure_case_1.png" width="780">-->
<!--<p>Good case</p>-->
<!--<img src="./failure_case_1.png" width="780">-->
<!--<p>Failure case</p>-->
<!--</center>-->
<!--</div>-->

<!--<br>-->
<!--<hr/>-->

<div class="content">
    <h2>Explainer Video</h2>
    <center>
        <p>Below is a 2 minutes video briefly explaining our model and showing selected examples.</p>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/_xlEanOJ8NE" frameborder="0"
                allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </center>
</div>

<hr/>

<div class="content">
    <h2>Code and Masks</h2>
    <p>We open-source our Pytorch implementation on <a
            href="https://github.com/fabienbaradel/object_level_visual_reasoning">
        Github</a> and release the Mask-RCNN predictions used for the object head (resolution=100x100 and min
        confidence=0.5).

        You should first download the Mask-RCNN predictions on VLOG and EPIC-AR.
        For more informations please refer to the following website ->
        <a href="../masks_data/"> Complementary Mask Data</a>.
    </p>
</div>

<hr/>

<div class="content">
    <h2>Bibtex</h2>
    <pre>
        <tt>@InProceedings{Baradel_2018_ECCV,
                author = {Baradel, Fabien and Neverova, Natalia and Wolf, Christian and Mille, Julien and Mori, Greg},
                title = {Object Level Visual Reasoning in Videos},
                booktitle = {ECCV},
                month = {June},
                year = {2018}
                }
        </tt>
    </pre>


</div>

<hr/>


<div class="content">
    <h2>Acknowledgements</h2>
    <p>
        This work was supported by the <a href="http://liris.cnrs.fr/deepvis/wiki/doku.php?id=start#publications">ANR/NSREC
        DeepVision project</a>
    </p>


</div>


</body>
</html>